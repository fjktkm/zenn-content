---
title: "ベクターフォントの機械学習用ライブラリ TorchFont を作った"
emoji: "🧪"
type: "tech"
topics: ["pytorch", "font", "機械学習", "maturin"]
published: true
---

## 1. はじめに

フォントは印刷物やデジタルデバイスにおいて文字情報を伝達するために重要な役割を果たしています．その制作には多くの時間と労力が必要であり，機械学習による自動生成が期待されています．

既存のフォントに関する機械学習手法は，主に画像生成技術の応用として発達してきました．代表的なものは VAE や GAN を用いた手法であり，これらはビットマップフォントの生成に成功しています．しかし，フォントは様々なスケールで利用されるため解像度に依存しないベクター形式が主流であり，ビットマップ形式での生成には実用性が低いという課題があります．

ベクターフォントの生成の研究があまり進展していない背景のひとつには，**ベクターフォント特有のデータ構造に対応した機械学習用ライブラリが不足している** ことが挙げられます．そこで，わたしは PyTorch ベースのベクターフォント用ドメイン特化ライブラリ TorchFont を開発しました．この記事では，TorchFont の設計理念や機能について紹介します．

## 2. 既存手法

### 2.1. DeepSVG のデータセット

ベクター画像の生成に取り組んだ初期の研究に DeepSVG があります．DeepSVG ではベクター画像だけでなくベクターフォントの生成にも取り組んでおり，ソースコードとともにデータセットが公開されています（厳密にはその先行研究の SVG-VAE のデータセットです）．

https://alexandre01.github.io/deepsvg/

https://github.com/alexandre01/deepsvg

ここで，DeepSVG のフォントデータセットと，Google Fonts のデータセットを比較すると次のようになります．

|              | フォントフェイス数 |   文字種数 |  総サンプル数 |
| :----------: | -----------------: | ---------: | ------------: |
|   DeepSVG    |             45,821 |         62 |       100,000 |
| Google Fonts |           約 9,000 | 約 110,000 | 約 12,000,000 |

Google Fonts はフォント数こそ少ないものの，サンプル数において約 120 倍もの差があります．また，DeepSVG のデータセットは英数字のみであるのに対し，Google Fonts にはそうした制限がありません．特に日本語を含む多言語フォントの生成を考えると，DeepSVG のデータセットでは不十分だと言わざるを得ないでしょう．

### 2.2. TTFQuery および fontTools

ベクターフォントのアウトラインを取り出すためのライブラリとして TTFQuery があります．TTFQuery は Python の fontTools をラップしたライブラリであり，フォントのアウトライン情報を簡単に取得できます．

https://github.com/mcfletch/ttfquery

そもそも fontTools はフォント開発の現場でも使用される汎用的で強力なライブラリであり，フォントの解析や編集など様々な機能を提供しています．別に TTFQuery を使用せずとも，fontTools を直接利用することでフォントのアウトライン情報を取得することも可能です．詳しくは fontTools の Pen プロトコルなどを参照するとよいでしょう．

https://github.com/fonttools/fonttools

一方で，fontTools を使用した方法には問題点があります．問題点は大きく次の 2 つです．

- メモリ使用量が多い
- パーサーの処理速度が遅い

具体的には，Google Fonts に含まれるすべてのフォントを読み込むのに，**メモリを 30 GB 程度要します**．これでパース速度が速ければ毎回パースしてもよいのですが，処理速度も遅いためメモリに保持し続ける必要があります．

また，フォントファイルのパース結果はマルチプロセスで共有できないため，PyTorch の Dataloader で `num_workers` を増やす場合 `num_workers` 倍のメモリが必要になります．これが特に致命的で，マシンのメモリ容量が限られている場合，Dataloader がボトルネックになってしまいます．DeepSVG などにおいてオンザフライで処理せずあらかじめデータセットを構築しているのも，このあたりの問題が原因なのではないかと推測しています．

これらは fontTools が汎用的であるがゆえの問題点で fontTools に文句を言うのは筋違いですが，とはいえ機械学習の分野で使用するにはやや不向きな面があることは否めません．

## 3. 提案手法

### 3.1. Skrifa

フォント関連のライブラリには，大きく次の 2 つの潮流がありました．

- FreeType や HarfBuzz などの C/C++ ベースのライブラリ
- fontTools などの Python ベースのライブラリ

C/C++ ベースのライブラリは，主にフォントのレンダリングなどのユーザー向けの処理を担う一方，Python ベースのライブラリはフォントの編集などのフォントの開発者向けの処理を担っていました．

こうした状況に対して，近年 Python より高速で C/C++ よりメモリ安全な Rust を用いてリプレースするプロジェクトが進められています．それが Google Fonts チームによる Oxidize プロジェクトです．

https://github.com/googlefonts/oxidize

たとえば，Chrome では以前はフォントのレンダリングに C/C++ ベースの FreeType が使用されていましたが，2025 年に Rust ベースの Skrifa に置き換えられました．

https://developer.chrome.com/blog/memory-safety-fonts?hl=ja

TorchFont では，アウトラインの取得に Skrifa を採用しました．Skrifa は fontTools と比較してパース速度が極めて高速で，パース結果を常にメモリに保持し続ける必要性がなくなりました．

その結果，メモリ使用量が大幅に削減されました．具体的には，fontTools では Google Fonts 全体をメモリに読み込むのに 30 GB 程度必要だったのに対して，Skrifa を使用した場合では **2 GB 程度まで削減**されました．

### 3.3. Memory Mapped I/O

Skrifa の採用は，マルチプロセスでのメモリ共有を可能にした点でも重要です．というのも，Skrifa の採用によりマルチプロセスにおいてもパース結果の共有を考える必要がなくなり，単にフォントファイルの共有を考えればよくなったからです．

フォントファイルなら，Memory Mapped I/O で読み込むことでマルチプロセスでの共有を簡単に実現できます．この方法により，メモリ使用量の増大を伴わずに PyTorch の Dataloader の `num_workers` を大きくすることが可能になりました．

https://crates.io/crates/memmap2

### 3.4. Maturin

TorchFont は機械学習用ライブラリであるため，Python から利用できることが望ましいです．そこで，TorchFont は Maturin を用いて Python と Rust の混成プロジェクトとして実装しました．

Maturin とは，Rust のソースコードを Python から呼び出すためのツールです．Maturin を用いることで，Rust で Skrifa を利用して取得したフォントのアウトライン情報を Python で扱えるようになるというわけです．

https://www.maturin.rs/

## 4. 使用例

実際に作成したライブラリは以下のリポジトリで公開しています．

https://github.com/fjktkm/torchfont

TorchFont を使用して，Google Fonts のフォントを読み込みアウトライン情報を取得する例を以下に示します．

### 4.1. インストール

TorchFont は PyPI で公開しているため，`pip` コマンドでインストールできます．

```bash
pip install torchfont
```

また，最近シェアが拡大している `uv` の場合は以下のコマンドでインストールできます．

```bash
uv add torchfont
```

### 4.2. データセットの構築

TorchFont では，もっとも頻繁に使用されるであろう Google Fonts について簡単にデータセットを構築できるようにデータセットクラスを用意しています．次のようなコードを実行することで PyTorch の Dataset クラスを継承した Google Fonts のデータセットを構築できます．

```python
from torchfont.datasets import GoogleFonts

dataset = GoogleFonts(
    root="data/google/fonts",
    ref="main",
    download=True,
)
```

`GoogleFonts` クラスの引数には，次のようなものを指定できるようにしてあります．

- `root`: Google Fonts のリポジトリをクローンするディレクトリ
- `ref`: クローンするブランチやタグ
- `patterns`: データセットに使用するフォントファイルのパスのパターン
- `codepoint_filter`: データセットに使用する文字の Unicode のリスト
- `transform`: 取り出したアウトライン情報に対して適用する変換関数
- `download`: Google Fonts のリポジトリをクローンするかどうか

### 4.3. アウトライン情報の取得

データセットを構築したら，あとは PyTorch の通常の Dataset と同様にインデックスでアクセスするだけでアウトライン情報を取得できます．次に例を示します．

```python
types, coords, style_label, content_label = dataset[42]

print(f"{types=}")
print(f"{coords=}")
print(f"{style_label=}")
print(f"{content_label=}")
```

データセットの返り値は，次の 4 つの要素からなるタプルとなります．上記のコードの実行結果は次のようになります．

```python
types=tensor([1, 3, 3, 3, 3, 2, 3, 3, 3, 3, 2, 2, 4, 5])
coords=tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.2759,  0.2207],
        [ 0.2759,  0.1755,  0.2701,  0.1295,  0.2585,  0.0828],
        [ 0.2470,  0.0361,  0.2309, -0.0092,  0.2102, -0.0530],
        [ 0.1895, -0.0968,  0.1647, -0.1377,  0.1357, -0.1758],
        [ 0.1068, -0.2139,  0.0749, -0.2471,  0.0400, -0.2754],
        [ 0.0000,  0.0000,  0.0000,  0.0000, -0.0352, -0.2300],
        [-0.0156, -0.2033,  0.0015, -0.1730,  0.0161, -0.1392],
        [ 0.0308, -0.1053,  0.0430, -0.0701,  0.0527, -0.0337],
        [ 0.0625,  0.0028,  0.0698,  0.0394,  0.0747,  0.0762],
        [ 0.0796,  0.1130,  0.0820,  0.1479,  0.0820,  0.1812],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0820,  0.7031],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.2759,  0.7031],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]])
style_label=0
content_label=74
```

`types` は描画コマンドの種類を表すクラスラベルのテンソルです．`0` から `5` までの整数値をとり，それぞれ次の描画コマンドを表します．

| クラスラベル | 描画コマンド |
| :----------: | :----------: |
|      0       |    `PAD`     |
|      1       |   `MoveTo`   |
|      2       |   `LineTo`   |
|      3       |  `CurveTo`   |
|      4       | `ClosePath`  |
|      5       |    `EOS`     |

`coords` は描画コマンドの引数にあたる制御点と終点の座標を表すテンソルです．`MoveTo` と `LineTo` の場合は終点のみで 2 次元，`CurveTo` の場合は制御点が 2 つと終点があるため 6 次元，その他は引数を持ちません．最大の 6 次元に合わせてテンソルが構成されており，関係のない部分はすべて 0 埋めされています．値の範囲については，各フォントの持つ `UPEM` の値で正規化し，おおむね 0 から 1 に収まるようにしています．

`style_label` はフォントフェイスのクラスを表す整数値，`content_label` は文字種のクラスを表す整数値です．スタイルとコンテンツという 2 つのラベルがある点がフォントというドメインの特徴的な点だと言えます．どちらも 0 から始まる連番で表現されているので注意が必要です．たとえば `content_label` は Unicode コードポイントとは一致しません．

### 4.4. データセットの情報取得

PyTorch や TorchVision の既存のデータセットと同じような使い心地が実現できるように，データセットクラスにデータセットの長さやクラスラベルを取得するためのプロパティを用意しています．以下に例を示します．

```python
print(f"{len(dataset)=}")
print(f"{len(dataset.content_classes)=}")
print(f"{len(dataset.style_classes)=}")

print(f"{dataset.style_classes[0:2]=}")
print(f"{dataset.content_classes[42:58]=}")
```

実行結果は次のようになります．

```python
len(dataset)=12076302
len(dataset.content_classes)=113545
len(dataset.style_classes)=8772
dataset.style_classes[0:2]=['Aclonica Regular', 'Arimo Italic Italic']
dataset.content_classes[42:58]=['*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9']
```

このように，クラスのインデックスからフォントフェイス名や文字種の情報を簡単に取得することができます．

### 4.5. `DataLoader` によるバッチ処理

TorchFont のデータセットは PyTorch の Dataset クラスを継承しているため，通常の Dataset と同様に `DataLoader` に渡してバッチ処理を行うことができます．

`types` と `coords` は描画コマンドと対応する可変長のシーケンスであるため，テキスト生成などと同様に `collate_fn` でパディングを行う必要があります．

```python
from torch.nn.utils.rnn import pad_sequence

def collate_fn(
    batch: Sequence[tuple[Tensor, Tensor, int, int]],
) -> tuple[Tensor, Tensor, Tensor, Tensor]:
    types_list = [types for types, _, _, _ in batch]
    coords_list = [coords for _, coords, _, _ in batch]
    style_label_list = [style for _, _, style, _ in batch]
    content_label_list = [content for _, _, _, content in batch]

    types_tensor = pad_sequence(types_list, batch_first=True, padding_value=0)
    coords_tensor = pad_sequence(coords_list, batch_first=True, padding_value=0.0)

    style_label_tensor = torch.as_tensor(style_label_list, dtype=torch.long)
    content_label_tensor = torch.as_tensor(content_label_list, dtype=torch.long)

    return types_tensor, coords_tensor, style_label_tensor, content_label_tensor
```

このような `collate_fn` を用いて `DataLoader` を構築します．たとえば，次のように実装するとバッチサイズ 64，ワーカープロセス数 8 でデータを読み込むことができます．

```python
from tqdm import tqdm
from torch.utils.data import DataLoader

dataloader = DataLoader(
    dataset,
    batch_size=64,
    shuffle=True,
    num_workers=8,
    prefetch_factor=2,
    collate_fn=collate_fn,
)

for batch in tqdm(dataloader, desc="Iterating over datasets"):
    sample = batch
```

こちらのコードを実行すると，Google Fonts を高速に読み込む様子が確認できると思います．オンザフライでここまで高速かつ小さいメモリ使用量でベクターフォントのアウトライン情報を取得できるライブラリはほかに存在しないと思います．

### 4.6. カスタムデータセットの構築

TorchFont では，Google Fonts 以外のフォントリポジトリからもデータセットを構築できるように汎用的な `FontRepo` クラスも用意しています．

たとえば Material Design Icons や Font Awesome はアイコンのプロバイダですが，実はこれらの GitHub のリポジトリにはアイコンをフォント形式にしたものが用意されています．そのため，`FontRepo` クラスを用いてアイコンのデータセットを構築することも可能になります．

たとえば，Google の Material Design Icons のデータセットは次のように構築できます．

```python
dataset = FontRepo(
    root="data/google/material_design_icons",
    url="https://github.com/google/material-design-icons",
    ref="master",
    patterns=("variablefont/*.ttf",),
    download=True,
)
```

また，Font Awesome のアイコンのデータセットは次のように構築できます．

```python
dataset = FontRepo(
    root="data/fortawesome/font-awesome",
    url="https://github.com/FortAwesome/Font-Awesome",
    ref="7.x",
    patterns=("otfs/*.otf",),
    download=True,
)
```

安易に `numpy` のバイナリファイルなどに変換してしまわず，フォントファイルというデータ形式にこだわったことの威力が，ここで発揮されていると言えるでしょう．

## 5. 考察

### 5.1. Google Fonts の注意点

ちなみに素朴に Google Fonts をすべて使おうとすると様々な罠があります．たとえば Google Fonts のリポジトリには Adobe Blank というフォントが含まれているのですが，これはフォールバックの動作確認用のフォントで，すべてのコードポイントに空の文字が割り当てられています．こちらはデフォルトで除外しました．

https://github.com/google/fonts/tree/main/ofl/adobeblank

ほかにも厄介なフォントに Rubik Pixels があります．Google Fonts のページを見てもらうのが手っ取り早いのですが，Rubik Pixels は非常に複雑なアウトラインを持っています．具体的には，Google Fonts におけるアウトラインの系列長の平均がだいたい 85 くらいなのに対して，Rubik Pixels は最大で 33,000 もあります．

除外はやりすぎかと考え実施していませんが，実用上は系列長に上限を設けるなどの対策を講じたほうがよいでしょう．もし対策をしないと，学習の途中でメモリ不足が発生し学習が止まります．場合によっては消費電力のスパイクでマシンが落ちることがあります（経験談）．

https://fonts.google.com/specimen/Rubik+Pixels

Google Fonts の統計的な情報については以下のリポジトリで色々と可視化しているのでぜひ参考にしてください．

https://github.com/fjktkm/google-fonts-heatmap

### 5.2. 対応フォーマット

地味なこだわりポイントとして，バリアブルフォントとフォントコレクション（`.ttc`，`.otc`）にも対応している点があります．これにより，どんなフォントリポジトリでも基本的に問題なく対応できるようになっていると思います．

なお，バリアブルフォントについては，`NamedInstance` ごとに異なるフォントフェイスとして扱うようにしています．バリアブルフォントなので本当はもっとデータセットを水増しすることはできるのですが，バリアブルフォント以外のフォントとの整合性を取るためこのような取り扱いにしています．データ拡張の方法としてバリアブルフォントの軸を用いること自体は可能だと思うので，今後の課題として検討したいと考えています．

### 5.3. ラベル設計

フォントのウェイト（太さ）やイタリック体かどうかなどの情報は，現在のところラベルに追加していません．というのも，こうした情報を統一的に扱う方法が現状見つけられていないからです．たとえば，スタティックフォントとバリアブルフォントではウェイトの情報の扱い方が異なる場合があります．

また，そもそもの話としてフォントファイルには様々なメタデータが含まれており，それらをどのようにラベルとして扱うかも検討が必要です．すべてのメタデータを取得できるようにするには膨大な Python バインディングを作成する必要があり，これは別のプロジェクトにしても差し支えないほどの規模になります．今後の課題として検討していきたいと考えています．

### 5.4. モデルの追加

TorchFont は TorchVision や TorchAudio のようなライブラリを目指しており，既存のモデルの実装についても追加していくことを検討しています．ただし，なにぶんこの分野はまだ前処理の定石のようなものが整っておらず，研究者ごとにかなり前処理に差異が見られます．しばらくは上手な実装を模索しつつという形になりそうです．

## 6. おわりに

この記事では，ベクターフォントの機械学習のためのドメイン特化ライブラリ TorchFont を紹介しました．Rust ベースの Skrifa を採用することで，従来の fontTools ベースの方法と比較して大幅にメモリ使用量とパース速度を改善できました．また，PyTorch の Dataset をベースに実装し，既存の PyTorch エコシステムとシームレスに統合できるようにしました．まだ発展途上のライブラリですが，ベクターフォントの生成に関心のある研究者や開発者の方々にとって有用なツールとなると思います．ぜひ使ってみてください．

## ソースコードの公開状況

TorchFont のソースコードは以下のリポジトリで公開しています．バグ報告や機能追加の提案は歓迎ですので，Issue や Pull Request でお知らせください．

https://github.com/fjktkm/torchfont

ドキュメントについては，まだ作りかけではありますが以下の URL で公開しています．

https://torchfont.readthedocs.io/ja/latest/
